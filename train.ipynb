{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "import torch\n",
    "\n",
    "from model.trainer import ChatTrainer\n",
    "from config import TrainConfig, T5ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainConfig(epochs=8, batch_size_per_gpu=16, learn_rate=0.0001, div_factor=50, mixed_precision='bf16', gradient_accumulation_steps=8, warmup_steps=1024, tokenizer_dir='/Users/chaofeng/code/ChatLM-mini-Chinese/model_save/', model_file='/Users/chaofeng/code/ChatLM-mini-Chinese/model_save/chat_small_t5.{}.bin', model_config_file='/Users/chaofeng/code/ChatLM-mini-Chinese/model_save/model_config.json', train_file='/Users/chaofeng/code/ChatLM-mini-Chinese/data/my_train_dataset.parquet', validation_file='/Users/chaofeng/code/ChatLM-mini-Chinese/data/my_valid_dataset.parquet', test_file='/Users/chaofeng/code/ChatLM-mini-Chinese/data/my_test_dataset.parquet', train_state_dir='/Users/chaofeng/code/ChatLM-mini-Chinese/model_save/train_latest_state', output_dir='/Users/chaofeng/code/ChatLM-mini-Chinese/model_save/pretrain', logging_steps=50, save_steps=10000, keep_latest_n_ckp=8, seed=23333, dataloader_buffer_size=50000, max_seq_len=256)\n",
      "T5ModelConfig(d_ff=3072, d_model=768, num_heads=12, d_kv=64, num_decoder_layers=10, num_layers=10)\n",
      "gpu device count: 0\n"
     ]
    }
   ],
   "source": [
    "train_config = TrainConfig()\n",
    "model_config = T5ModelConfig()\n",
    "\n",
    "print(train_config)\n",
    "print(model_config)\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print('gpu device count: {}'.format(gpu_count))\n",
    "\n",
    "chat_trainer = ChatTrainer(train_config=train_config, model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-28 08:53:41.254] [INFO]: \u001b[32mcpu memory available: 1.69 GB, disk space available: 41.63 GB, keep dataset in memory: False.\u001b[0m\n",
      "[2024-04-28 08:53:41.256] [INFO]: \u001b[32moperation: train, keep training: False, loading datasets ...\u001b[0m\n",
      "[2024-04-28 08:53:41.314] [INFO]: \u001b[32musing device: mps \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on MPS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-28 08:53:44.057] [INFO]: \u001b[32mtrain dataset size: 3000, steps per epoch:187; validation dataset size: 1000, steps per validation: 62; datalodater num_workers: 0.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "User specified an unsupported autocast device_type 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m train_args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 使用notebook_launcher函数启动多卡训练\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixed_precision\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python311-mini-LLM/lib/python3.11/site-packages/accelerate/launchers.py:207\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 207\u001b[0m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/ChatLM-mini-Chinese/model/trainer.py:261\u001b[0m, in \u001b[0;36mChatTrainer.train\u001b[0;34m(self, is_keep_training, is_finetune)\u001b[0m\n\u001b[1;32m    248\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain dataset size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, steps per epoch:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m; validation dataset size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, steps per validation: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m; datalodater num_workers: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(train_dataset), steps_per_epoch, \u001b[38;5;28mlen\u001b[39m(valid_dataset), eval_steps, num_workers), save_to_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    252\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR(\n\u001b[1;32m    253\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[1;32m    254\u001b[0m         max_lr\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mdiv_factor \u001b[38;5;241m*\u001b[39m train_config\u001b[38;5;241m.\u001b[39mlearn_rate, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         cycle_momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[0;32m--> 261\u001b[0m model, optimizer, lr_scheduler, train_dataloader, valid_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keep_training:\n\u001b[1;32m    270\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mload_state(input_dir\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mtrain_state_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/python311-mini-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:1213\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_megatron_lm(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python311-mini-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:1214\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_megatron_lm(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1214\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1215\u001b[0m     )\n\u001b[1;32m   1216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python311-mini-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:1094\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m   1096\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m~/anaconda3/envs/python311-mini-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:1280\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1278\u001b[0m model\u001b[38;5;241m.\u001b[39m_original_forward \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m   1279\u001b[0m model_forward_func \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mforward, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__func__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mforward\n\u001b[0;32m-> 1280\u001b[0m autocast_context \u001b[38;5;241m=\u001b[39m \u001b[43mget_mixed_precision_context_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnative_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast_handler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m new_forward \u001b[38;5;241m=\u001b[39m autocast_context(model_forward_func)\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mforward, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__func__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/python311-mini-LLM/lib/python3.11/site-packages/accelerate/utils/modeling.py:1543\u001b[0m, in \u001b[0;36mget_mixed_precision_context_manager\u001b[0;34m(native_amp, autocast_kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mautocast_kwargs)\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   1536\u001b[0m     DistributedType\u001b[38;5;241m.\u001b[39mNO,\n\u001b[1;32m   1537\u001b[0m     DistributedType\u001b[38;5;241m.\u001b[39mMULTI_CPU,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1541\u001b[0m     DistributedType\u001b[38;5;241m.\u001b[39mFSDP,\n\u001b[1;32m   1542\u001b[0m ]:\n\u001b[0;32m-> 1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mautocast_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mautocast_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/python311-mini-LLM/lib/python3.11/site-packages/torch/amp/autocast_mode.py:241\u001b[0m, in \u001b[0;36mautocast.__init__\u001b[0;34m(self, device_type, dtype, enabled, cache_enabled)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_device_mod\u001b[38;5;241m.\u001b[39mget_autocast_dtype()\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser specified an unsupported autocast device_type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m     )\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    246\u001b[0m     enabled\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mcommon\u001b[38;5;241m.\u001b[39mamp_definitely_not_available()\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: User specified an unsupported autocast device_type 'mps'"
     ]
    }
   ],
   "source": [
    "train = chat_trainer.train\n",
    "\n",
    "# chat_trainer.train() args:  is_keep_training: bool, is_finetune: bool\n",
    "train_args = (False, False)\n",
    "\n",
    "# 使用notebook_launcher函数启动多卡训练\n",
    "notebook_launcher(train, num_processes=gpu_count, args=train_args, mixed_precision=train_config.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = chat_trainer.test\n",
    "notebook_launcher(test, num_processes=gpu_count, mixed_precision=train_config.mixed_precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
